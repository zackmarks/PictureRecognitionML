# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fwXNgcp8GosPo2QpYeepFrSGiePEI8Kt
"""

#Zachary (Zack) Marks
#8872996
#teamname on codelab: zam

#I worked in Google colab, so there are some oddities that might be different when you test the data

# Commented out IPython magic to ensure Python compatibility.
#did a similar project in my cs190i class. many of these imports and other parts of the project are copied in from that
import os
import io
import random
import numpy as np
import pandas as pd
from tqdm import tqdm

from skimage.io import imread
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms

from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, CELU

from PIL import Image, ImageFile

from torch import optim, nn

import zipfile
from zipfile import ZipFile

#use_deterministic_algorithms()

# set the random seed for reproduction 
SEED=0
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# checking if GPU is available or not
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

with zipfile.ZipFile("./hw4_train.zip", 'r') as trainZip:
    trainZip.extractall()

"""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"""

trainingSize, validationSize = 0,0
trainDataLoader2 = 0

#load training and validation data, set 15% of the data to be val, 85% train

trainingImageSet = './hw4_train'
batchSize = 32
valSize = .15

#def load_data(training_dataset, batch_size):
# global training_loader
# global validation_loader

inputDataFolder = datasets.ImageFolder(trainingImageSet, transform = transforms.ToTensor())
trainSize, validationSize = torch.utils.data.random_split(inputDataFolder, [valSize, 1-valSize])
trainDataLoader = DataLoader(trainSize, batch_size = batchSize, shuffle = True)
validDataLoader = DataLoader(validationSize, batch_size = batchSize, shuffle = True)
print("sizes:",len(trainSize), len(validationSize), len(trainSize)+ len(validationSize))
trainDataLoader2 = DataLoader(trainSize, batch_size = batchSize, shuffle = True)
print(type(trainDataLoader2))

"""
print(type(trainDataLoader2))
path = '/content/hw4_train/0/0_0.png'
pic = imread(path)
print(type(pic))
#print(pic)
i = 0
plt.figure(figsize=(10,10))
plt.subplot(221), plt.imshow(pic, cmap='gray')
"""

class LeNet(nn.Module):#bad
    def __init__(self):
        super().__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution
        self.conv1 = torch.nn.Conv2d(3, 6, kernel_size = 5)
        self.conv2 = torch.nn.Conv2d(6, 16, kernel_size = 5)
        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)

        # figure out the input dimension of the first linear layer
        self.fc1 = torch.nn.Linear(16 * 4 * 4, 120)  
        self.fc2 = torch.nn.Linear(120, 84)
        self.fc3 = torch.nn.Linear(84, 10)

    def forward(self, x, target=None):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        if target is not None:
            loss = nn.CrossEntropyLoss()(out, labels)
            return out, loss
        return x


class CNN2(nn.Module):
    #max of .836 at epoch = 43, training rate = .0018
    #best sgd lr = .0017, epochs = 99 .    .838 submitted
    #best Adam lr = .0008 epochs = 15      0.8559290382819794
    def __init__(self):
        super(CNN2, self).__init__()
        self.cnn_layers = Sequential(
            Conv2d(3, 4, kernel_size=3, stride=1, padding=1),
            BatchNorm2d(4),
            torch.nn.CELU(inplace=True),
            MaxPool2d(kernel_size=2, stride=2),
            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),
            BatchNorm2d(4),
            torch.nn.CELU(inplace=True),
            MaxPool2d(kernel_size=2, stride=2),
        )
        self.linear_layers = Sequential(Linear(4 * 7 * 7, 10))

    def forward(self, x):
        x = self.cnn_layers(x)
        x = x.view(x.size(0), -1)
        x = self.linear_layers(x)
        return x



class cnn6(nn.Module):#best model
    #0.8698879551820728 15 epo, 0.0007 lr
    def __init__(self):
        super(cnn6, self).__init__()
        #use 28 x 28 x 3 for input
        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 4, stride = 1)
        #25 x 25 x 16
        self.conv2 = nn.Conv2d(16,32,4,1)
        #22 x 22 x 32

        #11*11*32
        self.fc1 = nn.Linear(3872,16*9)
        self.fc2 = nn.Linear(16*9,10)
    def forward(self,x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x,2)#stride is 2
        x = torch.flatten(x,1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x

usedCNN = cnn6().to(device)

# bigAcc = 0
# bigEpo = 0
def checkAgainstValData(epo, CNN):
    CNN.eval()
    lossFunct = nn.CrossEntropyLoss()
    with torch.no_grad():
        correctGuess = 0
        for images, labels in validDataLoader:
            images, labels = images.to(device), labels.to(device)
            predicted = CNN(images)
            correctGuess += (predicted.argmax(1) == labels).type(torch.float).sum().item()

    percAcc = correctGuess / len(validDataLoader.dataset)
    print("epoch ", epo , "percent accurate: ", percAcc)
    # if percAcc > bigAcc:
    #   bigAcc, bigEpo = percAcc, epo
    return epo, percAcc

# trains the neural network
from torch import optim, nn
# print(bigAcc, bigEpo)

#2 best sgd lr = .0017, epochs = 99 .    .838 submitted
#2 best Adam lr = .0008 epochs = 15      0.8559290382819794
maxEpochs = 25
learningRate = .0007
optimizer = optim.Adam(usedCNN.parameters(), lr = learningRate) 
usedCNN.train()


bestEpo, bestAcc = 0,0
for i in range(maxEpochs):
    lossAmt = 0
    print(i)
    for index, (images , labels) in enumerate(trainDataLoader2):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        ran = usedCNN(images) #run usedcnn
        lossFunct = nn.CrossEntropyLoss()
        loss = lossFunct(ran, labels)
        loss.backward()
        optimizer.step()
        lossAmt += loss.item()
    print(f"Training loss: {lossAmt/len(trainDataLoader2)}")
    currEpo, currAcc = checkAgainstValData(i, usedCNN)
    if currAcc > bestAcc and i >= 80:
        print("best updated")
        bestEpo = currEpo
        bestAcc = currAcc
        #bestCNN = copy.copy(usedCNN)
        # checkAgainstValData(i, bestCNN)

#15 - 84596 .0008
  
# print(bigAcc, bigEpo)

#model_scripted = torch.jit.script(usedCNN) # Export to TorchScript
#model_scripted.save('training.pt') # Save
#torch.save(usedCNN, 'model.pth')
torch.save(usedCNN.state_dict(), 'model.pth')

"""DELETE EVERYTHING BELOW HERE"""

# print(sortTupVec)
# #879838

print("aah")